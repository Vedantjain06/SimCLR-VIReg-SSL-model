{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - In the self-supervised pre\n",
        "training phase, a pre-defined pretext task is formulated for\n",
        " the deep learning algorithm to solve.\n",
        "\n",
        " Step 2-  Pseudo-labels for the\n",
        " pretext task are automatically generated based on specific\n",
        " attributes of the input data.\n",
        "\n",
        " Step 3-  Once the self-supervised pretraining process is completed, the acquired model can be\n",
        " transferred to downstream tasks.\n"
      ],
      "metadata": {
        "id": "WIFsohoUOT_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretask**\n",
        "\n",
        "Four categories of pretext tasks: context-based methods,\n",
        " CL, generative algorithms and contrastive generative methods.\n",
        "\n",
        " A)Context based ~\n",
        "\n",
        " 1) Rotation- The network learns to recognize general image features that are useful beyond rotations.These features (edges, textures, spatial layouts) are also essential for distinguishing different objects.\n",
        "\n",
        " 2)Colourization -  Upon completing the\n",
        " training process, the ab color channels would be predicted\n",
        " for any grayscale image. Consequently, the lightness chan\n",
        "nel and the ab color channels can be concatenated to restore\n",
        " the original grayscale image to a colorful representation\n",
        "\n",
        " 3)Jigsaw -Representation quality was found to improve with higher\n",
        "capacity models and increased problem complexity.\n",
        "\n",
        "B) Contrastive Learning ~The underlying principle is to promote proximity betweenpositive examples and maximize the separation between negative examples within the latent space.  \n",
        "\n",
        "1)MoCo - In MoCo (Momentum Contrast), contrastive learning (CL) is treated like a dictionary look-up task. This approach differentiates between positive and negative examples based on the similarity of representations for different views of an instance.\n",
        "\n",
        "2)simCLR- SimCLR takes a mini-batch of images and applies different random augmentations to each image, like cropping, color distortion, or flipping. Any other images in the mini-batch (like images of a cat or a car) are treated as \"negatives\".  SimCLR v1 does not explicitly select negative\n",
        " instances. Instead, for a given positive pair, the remaining\n",
        " 2(N −1).\n",
        "\n",
        "  MoCov2 builds upon MoCo v1  and SimCLR v1\n",
        " , incorporating a multilayer perceptron (MLP) projection\n",
        " head and more data augmentations.\n",
        "\n",
        "***augmentation done in simCLR are good for categorization tasks but not for obejct recognization tasks. so strong aug. is must but relying on that only is not good as well. The distortions introduced by strong\n",
        " data augmentation can alter the image structure, resulting\n",
        " in a distribution that differs from that of weakly augmented\n",
        " images***\n",
        "\n",
        "\n",
        " 3) Self-distillation-based CL(No negative pairs) ~\n",
        "\n",
        " Bootstrap Your Own Latent (BYOL) - These methods use two networks with the same structure, known as Siamese networks. One is called the \"online network\" (which is constantly updated), and the other is the \"target network\" (which slowly adopts the online network's changes).\n",
        "\n",
        " Both augemented versions go through each of the networks to produce representations (feature vectors).\n",
        "\n",
        "Online network updated on each turn whereas target network updates slowly This slow update prevents the target network from changing too quickly and helps the model avoid a \"collapse,\" where both networks could produce the same output for every input.\n",
        "\n",
        "SimSiam adds a unique component called a \"stop-gradient\" operation. Essentially, this means it treats the output from the target network as a constant in one direction of the update, which further reduces the chance of collapse and makes training more stable.\n",
        "\n",
        "**now everything is good but feature may not be diverse i.e model may not be using its full power ryt now and so we'll introduce barlow twins**\n",
        "\n",
        "\n",
        "Barlow Twins is a self-supervised learning (SSL) method that uses Reduce Reductancy approach\n",
        "Barlow Twins’ objective is to make each dimension of the embedding vector carry unique information, reducing redundancy. This results in more robust representations. to avoid squeezing of versions in small dimensions like maybe only cropped images or only rotated images\n",
        "\n",
        "Now, let's go through the loss function, which has two parts: increasing similarity and reducing redundancy.\n",
        "\n",
        "a)Increase Similarity: Make the embeddings za and zb as similar as possible along corresponding dimensions (i.e., the same \"feature\" in both views).\n",
        "b)Reduce Redundancy: Ensure different dimensions of the embeddings aren't too similar to each other, so that each dimension contains unique information.\n",
        "\n",
        "**now we'll add another civeg to barlow twins for better result and avoid collapse**\n",
        "\n",
        " VICReg builds on Barlow Twins' approach but introduces a new concept of variance regularization to prevent what’s called collapse . VICReg balances three key objectives:*** variance(spread out data), invariance(focus on similarity too), and covariance(reduce reductancy)*** 3 funts ▶\n",
        "\n",
        "funt 1)  The variance term uses a hinge function to maintain a minimum standard deviation for each dimension in\n",
        "za nd zb . If the standard deviation is below a set threshold\n",
        "γ, it adds a penalty to the loss.(not same ear crop for all animals)\n",
        "\n",
        "funt2)VICReg uses the mean squared Euclidean distance between za nd zb to measure the difference between the embeddings. The smaller the distance, the more similar they are.\n",
        "\n",
        "funt2)VICReg calculates the covariance matrix C of the embedding vectors and penalizes the off-diagonal elements (i.e., correlations between different dimensions). This encourages the dimensions to be independent of each other.(2 features should be varied diffrently like fur and ear crop)\n",
        "\n",
        "**CL is wide and other methods like VITs(multiple crops with entropy error) and patches(create negative and positive patches) show that it can help create relation between embeddigs even in supervised learning**\n",
        "\n",
        "\n",
        "c)**Generative Algorithm**\n",
        "\n",
        "MIM(masked image modelling)- Masked Image Modeling (MIM) involves taking an image, hiding (or masking) parts of it, and asking the model to predict the missing pieces. This teaches the model to understand the structure of images better.\n",
        "\n",
        "2 major algos- MAE and BeiT\n",
        "\n",
        "Mae-  This is a simpler model that hides large parts of an image (up to 75%) and uses a decoder to reconstruct the missing areas directly from the pixels.They use image directly as pixels. In this major portion of image is not there 75%. **Used on small dtatsets generally**. generally efficienct and accurate for less complex and less avability of data tasks\n",
        "\n",
        "BEiT: Like BERT in NLP, BEiT splits the image into smaller parts (tokens) and hides some. Then, it tries to predict these missing parts using visual tokens. they use pixel target tokens are target tokens to train model. they usually work on small portion of image gone like 25%.**used on larger datasets**. generally efficienct and accurate for  complex and more avability of data tasks\n",
        "\n",
        "d)**Contrastive Generative Algorithms**\n",
        "\n",
        "Contrastive models are data hungry and leads to overfitting whereas Generative algo leads to inferior data scaling capabilities . so we needed combination of these two models . 5 major models on this are:\n",
        "\n",
        "1)Using GANs for Data Augmentation in CL -  A Generative Adversarial Network (GAN) creates new, altered views of images to help a contrastive learning model learn view-invariant features\n",
        "\n",
        "2)iBOT: Inspired by BEiT and DINO -  iBOT combines the tokenization process from BEiT with cross-view distillation from DINO. This approach uses a teacher model (a stronger, pretrained model) to guide the learning of a new model, helping it achieve better accuracy on linear probing tasks\n",
        "\n",
        "3)RePre: Adding Local Feature Learning -  RePre adds a branch to the model specifically for reconstructing the original image pixels. This helps the model learn finer local details\n",
        "\n",
        "4)CMAE: Combining CL and MIM - CMAE combines Contrastive Learning (CL) and Masked Image Modeling (MIM) with two key innovations: \"pixel shifting\" and a \"feature decoder\n",
        "\n",
        "5)SiameseIM: Views from CL as MIM Targets - use on augementation by cl as the partially masked image for other augementation to learn\n",
        "\n",
        "**Recon is best of these and can be used**(Repre and CMAE are best as in this reconstruction and CL is done simultaneously.\n",
        "\n",
        "\n",
        "GAN(unsupervised learning method) but we will study this because we can surge it with our model -   The integration of GAN swith SSL offers various avenues, with self-supervised GANs (SS-GAN)\n",
        "2 components - Generator(generate fake data and will try to make it as realistic as possible) and Discriminator(distinguish between images created and compete with generator)\n",
        "\n",
        "**Semi Supervised Learning**\n",
        " SL4 is combination of super and semi supervised learning -  In S4L, Self-Supervised Learning (SSL)[only work on unlabelled data] and Semi-Supervised Learning (SSL)[ work with both labelled and unlabelled data] techniques are combined to leverage both labeled and unlabeled data in training.\n",
        " so in SSL we work on both labelled and unlabelled data to drive the model.\n",
        " Loss is calculated by combining loss from both labelled data similarity and unlabelled dayta CL techniques.\n",
        "\n",
        " **Multi-instance learning (MIL)**(supervised model methhod) [ with time series aspect]\n",
        "\n",
        " bag of instances - contains segments of time series\n",
        "\n",
        " bag labelling - positive label if atleast one positive instance,negative otherwise\n",
        "\n",
        "Instance-Level and Bag-Level Prediction- The goal is often to classify the bag based on its contents. and **goal is to identify instances because of which the bag is labbeled positive.**\n",
        "\n",
        "**MIL is used in mostly cases where the the query is required in yes/no rather than details of where when?? / overall outcome depends on only a subset of instances within a bag.**\n",
        "\n",
        "\n",
        "in ssl it used MIL to aggregate multiple instances from segment of unlabelled data and recognize as a bag rather than indivisual instances\n",
        "\n",
        "**Multi-view/multi-modal(ality) learning**\n",
        "\n",
        "multiview- represents samadata in multiple views and helps model to understand data better . often used in ssl and semi sl where data is limited\n",
        "methods like co-training , cca and clustering is used in this.\n",
        "\n",
        "multimodal - used diffrent data source like text,image and video etc to train the model **( used in timeseries)**. used in premium quality task because have to deal with noises in all data sources. techniques involve - Fusion, Cross-modal learning and attention mechnaism.**(CLIP's advancements have significantly propelled multimodal learning)**\n",
        "\n",
        "\n",
        "**Cloud 3d and 2d is used to combine 2d and 3d data with triplet entropy loss*\n",
        "good method- Autoencoders (AEs) and Capsule Networks for Point Clouds - compress data then try to recreate it after modifying. also captures sapial relationships between the points of clud.\n",
        "\n",
        "\n",
        "\n",
        "**TTT - if the test and training data is very diffrent use this method** - Lets see how this test time training model works. \"combining with MAE it works even better because it predicts the missing info in data first.\"\n",
        "TTT manages the bias ( old data correction not exactly pasted on new data) and variance(new data training too much vary).it finds common ground to manage both bias and variance.   "
      ],
      "metadata": {
        "id": "NLooXty1k7Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Keee58hvlw-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downstream task**\n",
        "\n",
        "Evaluation of model only .how good or bad it is??\n",
        "\n",
        "Downstream Task Evaluation: You test the model on an animal classification task (like identifying cats, dogs, and birds). If it performs well, you know it learned useful features.\n",
        "\n",
        "\n",
        "Network Dissection: You examine the network and discover that certain parts of the model have learned to recognize animal fur, feathers, or even specific patterns like spots or stripes. This shows that SSL helped the model understand animal-specific features.\n",
        "**this includes disintegrating each neuron to see what model learned**"
      ],
      "metadata": {
        "id": "c1bMfuthHP_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Techniques like contrastive predictive coding (CPC) have been effective for learning useful time series representations by maximizing mutual information between segments of data."
      ],
      "metadata": {
        "id": "UBNunNLUExR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XxyQXMB0GMRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIM\n",
        "    # Initialize masks and targets\n",
        "    mask = torch.zeros(batch_size, seq_length, dtype=torch.bool)\n",
        "    masked_data = data.clone()\n",
        "    target = data.clone()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Randomly select indices to mask\n",
        "        mask_indices = np.random.choice(seq_length, num_masked, replace=False)\n",
        "        mask[i, mask_indices] = True\n",
        "        masked_data[i, :, mask_indices] = 0  # Masking by setting the segment to zero\n",
        "\n",
        "    return masked_data, mask, target"
      ],
      "metadata": {
        "id": "PyvGSU4EjkQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TimeSeriesEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TimeSeriesEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, output_dim, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        return x.squeeze(-1)  # Output shape: (batch_size, output_dim)\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x  # Output shape: (batch_size, output_dim)\n",
        "\n",
        "\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, ts_input_dim, ts_hidden_dim, ts_output_dim, text_input_dim, text_output_dim):\n",
        "        super(CLIPModel, self).__init__()\n",
        "        self.ts_encoder = TimeSeriesEncoder(ts_input_dim, ts_hidden_dim, ts_output_dim)\n",
        "        self.text_encoder = TextEncoder(text_input_dim, text_output_dim)\n",
        "\n",
        "    def forward(self, ts_data, text_data):\n",
        "        ts_embedding = self.ts_encoder(ts_data)\n",
        "        text_embedding = self.text_encoder(text_data)\n",
        "        return ts_embedding, text_embedding\n",
        "\n",
        "    def compute_loss(self, ts_embedding, text_embedding, temperature=0.07):\n",
        "        # Normalize embeddings\n",
        "        ts_embedding = F.normalize(ts_embedding, dim=1)\n",
        "        text_embedding = F.normalize(text_embedding, dim=1)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.matmul(ts_embedding, text_embedding.T) / temperature\n",
        "\n",
        "        # Labels for contrastive loss\n",
        "        batch_size = ts_embedding.size(0)\n",
        "        labels = torch.arange(batch_size).to(ts_embedding.device)\n",
        "\n",
        "        # Contrastive loss\n",
        "        loss_ts_to_text = F.cross_entropy(similarity_matrix, labels)\n",
        "        loss_text_to_ts = F.cross_entropy(similarity_matrix.T, labels)\n",
        "        return (loss_ts_to_text + loss_text_to_ts) / 2\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Parameters\n",
        "ts_input_dim = 3         # Number of channels in the time-series data\n",
        "ts_hidden_dim = 64       # Hidden dimension for time-series encoder\n",
        "ts_output_dim = 128      # Output embedding dimension\n",
        "text_input_dim = 512     # Input dimension for text embeddings (e.g., BERT embedding size)\n",
        "text_output_dim = 128    # Output embedding dimension\n",
        "\n",
        "# Model instantiation\n",
        "clip_model = CLIPModel(ts_input_dim, ts_hidden_dim, ts_output_dim, text_input_dim, text_output_dim)\n",
        "\n",
        "# Sample batch of data\n",
        "batch_size = 32\n",
        "sequence_length = 128\n",
        "ts_data\n"
      ],
      "metadata": {
        "id": "hXCZroo1lyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, hidden_dim, output_dim, seq_length):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(noise_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim * seq_length // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim * seq_length // 2, output_dim * seq_length)\n",
        "        self.output_dim = output_dim\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = torch.relu(self.fc1(z))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        return x.view(-1, self.output_dim, self.seq_length)\n",
        "\n",
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, seq_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim * seq_length, hidden_dim * seq_length // 2)\n",
        "        self.fc2 = nn.Linear(hidden_dim * seq_length // 2, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten time-series data\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.sigmoid(self.fc3(x))\n"
      ],
      "metadata": {
        "id": "JH7VdHwnpHRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning loop\n",
        "def fine_tune(model, dataloader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "        accuracy = 100 * correct_preds / total_preds\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader):.4f}, Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "kI7RzBNKu9w6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}